/**
 * ApneDoctors Voice AI Backend - WINDOWS-NATIVE VERSION
 *
 * ğŸªŸ 100% WINDOWS COMPATIBLE
 * Uses: Transformers.js Whisper (STT) + Ollama (LLM) + Piper TTS (TTS)
 *
 * Monthly cost: $0 (only server hosting ~$10-50/month)
 */

const express = require('express');
const WebSocket = require('ws');
const axios = require('axios');
const { spawn } = require('child_process');
const fs = require('fs');
const path = require('path');
const os = require('os');

// =============================================================================
// CONFIGURATION (WINDOWS-SAFE)
// =============================================================================

const CONFIG = {
  PORT: process.env.PORT || 3001,

  // Windows-native AI Services
  OLLAMA_BASE_URL: process.env.OLLAMA_BASE_URL || 'http://localhost:11500', // Ollama (Llama 3)

  // Transformers.js models (run in Node.js, no external servers)
  USE_TRANSFORMERS_JS: true, // Always true for Windows

  // Piper TTS (Windows binary)
  PIPER_MODEL_PATH: process.env.PIPER_MODEL_PATH || path.join(__dirname, 'models', 'en_US-lessac-medium.onnx'),

  // n8n Integration
  N8N_WEBHOOK_URL: process.env.N8N_WEBHOOK_URL,

  // Voice Settings
  TTS_LANGUAGE: 'en',

  // Safety
  MAX_CALL_DURATION: 30 * 60 * 1000,
};

// =============================================================================
// MASTER OPERATING PROMPT (SAME AS BEFORE - MEDICAL SAFETY FIRST)
// =============================================================================

const MASTER_PROMPT = `You are APNE, the official medical voice intelligence of ApneDoctors.

ğŸ§  IDENTITY
You are NOT a chatbot or assistant. You are a calm, precise, safety-first medical conversation agent.

ğŸ©º CORE MISSION
1. Understand the caller's intent and condition
2. Collect medically relevant information
3. Assess urgency (RED/YELLOW/GREEN triage)
4. Take the correct action
5. Protect patient safety
6. Escalate to humans when required

You NEVER:
- Diagnose diseases
- Prescribe medication
- Guess medical facts
- Argue with the caller
- Downplay danger

ğŸš¨ TRIAGE CLASSIFICATION
- RED â†’ Immediate danger (chest pain, breathing difficulty, severe bleeding, unconsciousness)
- YELLOW â†’ Needs doctor attention soon
- GREEN â†’ Non-urgent

ğŸš¨ RED TRIAGE BEHAVIOR (LOCKDOWN MODE)
If RED is detected:
1. STOP exploratory conversation
2. Say: "I'm concerned about what you're describing. Please stay with me. I'm arranging immediate medical help for you."
3. Trigger emergency workflow
4. Keep caller engaged calmly

ğŸ§¬ STYLE GUIDELINES
- Speak like a private hospital concierge
- Warm, grounded, confident
- Less words > more words
- Calm beats clever

ğŸ” SAFETY RULE
Patient safety > User satisfaction > Completion. Always.

RESPONSE FORMAT (JSON):
{
  "state": "GREETING|CONSENT|SYMPTOM_COLLECTION|TRIAGE|ACTION|HANDOFF|CLOSING",
  "message": "what to say to caller",
  "action": "trigger_emergency_workflow|schedule_appointment|handoff_human|null",
  "triage_level": "RED|YELLOW|GREEN|UNKNOWN",
  "escalate": boolean
}`;

// =============================================================================
// WINDOWS-NATIVE AI SERVICES
// =============================================================================

class WindowsVoiceAI {
  constructor() {
    this.ollamaReady = false;
    this.transformersReady = false;
    this.piperReady = false;
    this.initializeServices();
  }

  async initializeServices() {
    console.log('ğŸ”§ Initializing Windows-native AI services...');

    // Check Ollama (Llama 3)
    try {
      await axios.get(`${CONFIG.OLLAMA_BASE_URL}/api/tags`);
      this.ollamaReady = true;
      console.log('âœ… Ollama (Llama 3) ready');
    } catch (error) {
      console.log('âš ï¸  Ollama not ready. Start with: ollama serve');
      console.log('   Make sure Ollama is running on port 11500');
    }

    // Initialize Transformers.js (runs in Node.js)
    try {
      await this.initTransformersJS();
      this.transformersReady = true;
      console.log('âœ… Transformers.js Whisper ready');
    } catch (error) {
      console.log('âš ï¸  Transformers.js failed to load:', error.message);
      console.log('   Install with: npm install @xenova/transformers');
    }

    // Check Piper TTS
    try {
      await this.checkPiper();
      this.piperReady = true;
      console.log('âœ… Piper TTS ready');
    } catch (error) {
      console.log('âš ï¸  Piper TTS not ready. Download piper.exe and models');
      console.log('   https://github.com/rhasspy/piper');
    }
  }

  // Check if Piper is available
  async checkPiper() {
    return new Promise((resolve, reject) => {
      const piper = spawn('piper.exe', ['--help'], { stdio: 'ignore' });

      piper.on('close', (code) => {
        if (code === 0) {
          resolve(true);
        } else {
          reject(new Error('Piper not found'));
        }
      });

      piper.on('error', () => {
        reject(new Error('Piper executable not found in PATH'));
      });

      // Timeout after 5 seconds
      setTimeout(() => {
        piper.kill();
        reject(new Error('Piper check timeout'));
      }, 5000);
    });
  }

  // WINDOWS-SAFE: Speech-to-Text with Transformers.js Whisper
  async transcribeAudio(audioBuffer) {
    if (!this.asr) {
      throw new Error('Transformers.js Whisper not initialized');
    }

    try {
      // Save audio to Windows temp directory
      const tempDir = os.tmpdir();
      const tempPath = path.join(tempDir, `audio_${Date.now()}.wav`);

      fs.writeFileSync(tempPath, audioBuffer);

      // Use Transformers.js for transcription (runs in Node.js)
      const result = await this.asr(tempPath, {
        language: 'en',
        task: 'transcribe'
      });

      // Clean up temp file
      fs.unlinkSync(tempPath);

      return result.text || '';
    } catch (error) {
      console.error('Transformers.js transcription error:', error);

      // Fallback: Return empty string (caller can retry)
      return '';
    }
  }

  // LLM Processing with Ollama (Llama 3) - ALREADY WINDOWS-COMPATIBLE
  async processWithLLM(callState, userMessage) {
    try {
      const messages = [
        {
          role: 'system',
          content: MASTER_PROMPT
        },
        ...callState.conversationHistory.map(msg => ({
          role: msg.role === 'caller' ? 'user' : 'assistant',
          content: msg.content
        })),
        {
          role: 'user',
          content: userMessage
        }
      ];

      const response = await axios.post(
        `${CONFIG.OLLAMA_BASE_URL}/api/chat`,
        {
          model: 'llama2',  // or llama3.3, mistral, etc.
          messages: messages,
          stream: false,
          format: 'json',  // Request JSON response
          options: {
            temperature: 0.7,
            top_p: 0.9,
            num_predict: 512
          }
        }
      );

      const textResponse = response.data.message.content;

      // Parse JSON response
      try {
        return JSON.parse(textResponse);
      } catch {
        // Fallback if not JSON
        return {
          state: callState.state,
          message: textResponse,
          action: null,
          triage_level: 'UNKNOWN',
          escalate: false
        };
      }
    } catch (error) {
      console.error('Ollama error:', error);
      throw error;
    }
  }

  // WINDOWS-SAFE: Text-to-Speech with Piper (Windows binary)
  async synthesizeSpeech(text) {
    return new Promise((resolve, reject) => {
      // Use Windows temp directory
      const tempDir = os.tmpdir();
      const outputPath = path.join(tempDir, `tts_${Date.now()}.wav`);

      // Spawn Piper process (Windows binary)
      const piper = spawn('piper.exe', [
        '--model', CONFIG.PIPER_MODEL_PATH,
        '--output_file', outputPath
      ]);

      // Send text to stdin
      piper.stdin.write(text);
      piper.stdin.end();

      piper.on('close', (code) => {
        if (code === 0) {
          try {
            const audio = fs.readFileSync(outputPath);
            fs.unlinkSync(outputPath); // Clean up
            resolve(audio);
          } catch (readError) {
            reject(new Error(`Failed to read Piper output: ${readError.message}`));
          }
        } else {
          reject(new Error(`Piper failed with code ${code}`));
        }
      });

      piper.on('error', (error) => {
        reject(new Error(`Piper process error: ${error.message}`));
      });

      // Timeout after 30 seconds
      setTimeout(() => {
        piper.kill();
        reject(new Error('Piper TTS timeout'));
      }, 30000);
    });
  }

  // Initialize Transformers.js (loads models into Node.js)
  async initTransformersJS() {
    try {
      const { pipeline } = await import('@xenova/transformers');

      console.log('â³ Loading Transformers.js models... (first run may take time)');

      // Load Whisper for speech-to-text
      this.asr = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny');

      console.log('âœ… Transformers.js Whisper loaded (runs in Node.js!)');
    } catch (error) {
      console.error('Failed to initialize Transformers.js:', error);
      throw error;
    }
  }
}

// =============================================================================
// CALL STATE MANAGER (SAME AS BEFORE)
// =============================================================================

class CallStateManager {
  constructor(callId) {
    this.callId = callId;
    this.state = 'GREETING';
    this.mode = 'INBOUND';
    this.conversationHistory = [];
    this.symptoms = [];
    this.triageLevel = 'UNKNOWN';
    this.consentGiven = false;
    this.startTime = Date.now();
    this.metadata = {};
  }

  addMessage(role, content) {
    this.conversationHistory.push({ role, content, timestamp: Date.now() });
  }

  updateState(newState) {
    console.log(`[${this.callId}] State transition: ${this.state} â†’ ${newState}`);
    this.state = newState;
  }

  setTriageLevel(level) {
    if (['RED', 'YELLOW', 'GREEN'].includes(level)) {
      this.triageLevel = level;
      console.log(`[${this.callId}] Triage level set to: ${level}`);
    }
  }

  shouldEscalate() {
    return this.triageLevel === 'RED' ||
           this.state === 'HANDOFF' ||
           (Date.now() - this.startTime) > CONFIG.MAX_CALL_DURATION;
  }

  getSummary() {
    return {
      callId: this.callId,
      duration: Date.now() - this.startTime,
      state: this.state,
      mode: this.mode,
      triageLevel: this.triageLevel,
      symptoms: this.symptoms,
      conversationHistory: this.conversationHistory,
      metadata: this.metadata
    };
  }
}

// =============================================================================
// TOOLS (WINDOWS-SAFE)
// =============================================================================

class ToolExecutor {
  async triggerEmergencyWorkflow(callState) {
    console.log(`ğŸš¨ EMERGENCY TRIGGERED for call ${callState.callId}`);

    if (CONFIG.N8N_WEBHOOK_URL) {
      try {
        await axios.post(`${CONFIG.N8N_WEBHOOK_URL}/emergency`, {
          callId: callState.callId,
          triageLevel: 'RED',
          symptoms: callState.symptoms,
          timestamp: new Date().toISOString(),
          summary: callState.getSummary()
        });
        console.log('âœ… Emergency alert sent to n8n');
      } catch (error) {
        console.error('âŒ n8n emergency webhook failed:', error.message);
      }
    } else {
      console.log('âš ï¸  No n8n webhook configured - emergency workflow skipped');
    }

    return {
      success: true,
      action: 'emergency_initiated'
    };
  }

  async scheduleAppointment(callState) {
    console.log(`ğŸ“… Scheduling appointment for call ${callState.callId}`);

    if (CONFIG.N8N_WEBHOOK_URL) {
      await axios.post(`${CONFIG.N8N_WEBHOOK_URL}/schedule-appointment`, {
        callId: callState.callId,
        symptoms: callState.symptoms,
        timestamp: new Date().toISOString()
      });
    }

    return { success: true, action: 'appointment_scheduled' };
  }

  async handoffToHuman(callState, reason = '') {
    console.log(`ğŸ‘¤ Handing off to human for call ${callState.callId}`);

    if (CONFIG.N8N_WEBHOOK_URL) {
      await axios.post(`${CONFIG.N8N_WEBHOOK_URL}/human-handoff`, {
        callId: callState.callId,
        reason,
        summary: callState.getSummary()
      });
    }

    return { success: true, action: 'human_handoff' };
  }

  async logCallSummary(callState) {
    const summary = callState.getSummary();
    console.log(`ğŸ“Š Call Summary [${callState.callId}]:`);
    console.log(`   Duration: ${Math.round(summary.duration / 1000)}s`);
    console.log(`   Final State: ${summary.state}`);
    console.log(`   Triage Level: ${summary.triageLevel}`);
    console.log(`   Messages: ${summary.conversationHistory.length}`);

    // Optional: Send to n8n for analytics
    if (CONFIG.N8N_WEBHOOK_URL) {
      try {
        await axios.post(`${CONFIG.N8N_WEBHOOK_URL}/call-summary`, summary);
      } catch (error) {
        // Non-critical, just log
        console.log('Call summary logging failed:', error.message);
      }
    }
  }
}

// =============================================================================
// WEBSOCKET SERVER (WINDOWS-COMPATIBLE)
// =============================================================================

class VoiceWebSocketServer {
  constructor(server) {
    this.wss = new WebSocket.Server({ server });
    this.activeCalls = new Map();
    this.aiEngine = new WindowsVoiceAI();
    this.toolExecutor = new ToolExecutor();

    this.wss.on('connection', (ws, req) => {
      const callId = this.generateCallId();
      const callState = new CallStateManager(callId);
      this.activeCalls.set(callId, { ws, callState });

      console.log(`ğŸ“ New call connected: ${callId}`);

      ws.on('message', async (data) => {
        await this.handleMessage(callId, data);
      });

      ws.on('close', () => {
        console.log(`ğŸ“ Call ended: ${callId}`);
        this.toolExecutor.logCallSummary(callState);
        this.activeCalls.delete(callId);
      });

      this.sendGreeting(callId);
    });
  }

  generateCallId() {
    return `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  async sendGreeting(callId) {
    const { ws, callState } = this.activeCalls.get(callId);

    try {
      const greeting = await this.aiEngine.processWithLLM(
        callState,
        '[SYSTEM: Initiate greeting for new caller]'
      );

      callState.addMessage('assistant', greeting.message);
      callState.updateState(greeting.state);

      const audioBuffer = await this.aiEngine.synthesizeSpeech(greeting.message);

      ws.send(JSON.stringify({
        type: 'audio',
        audio: audioBuffer.toString('base64'),
        text: greeting.message
      }));
    } catch (error) {
      console.error(`Greeting failed for ${callId}:`, error);
      ws.send(JSON.stringify({
        type: 'error',
        message: 'Unable to start conversation. Please try again.'
      }));
    }
  }

  async handleMessage(callId, data) {
    const { ws, callState } = this.activeCalls.get(callId);

    try {
      const message = JSON.parse(data);

      if (message.type === 'audio') {
        // Decode base64 audio
        const audioBuffer = Buffer.from(message.audio, 'base64');

        // Transcribe with Transformers.js Whisper
        const transcript = await this.aiEngine.transcribeAudio(audioBuffer);

        if (!transcript) {
          ws.send(JSON.stringify({
            type: 'error',
            message: 'I couldn\'t understand that. Could you please speak again?'
          }));
          return;
        }

        console.log(`[${callId}] User: ${transcript}`);
        callState.addMessage('caller', transcript);

        // Process with Ollama LLM
        const response = await this.aiEngine.processWithLLM(callState, transcript);

        callState.updateState(response.state);
        if (response.triage_level) {
          callState.setTriageLevel(response.triage_level);
        }

        // Execute any actions (emergency, scheduling, etc.)
        if (response.action) {
          await this.executeAction(callId, response.action, response);
        }

        // Check if should escalate
        if (response.escalate || callState.shouldEscalate()) {
          await this.toolExecutor.handoffToHuman(callState, 'AI escalation');
        }

        callState.addMessage('assistant', response.message);

        // Generate speech with Piper
        const audioBufferResponse = await this.aiEngine.synthesizeSpeech(response.message);

        ws.send(JSON.stringify({
          type: 'audio',
          audio: audioBufferResponse.toString('base64'),
          text: response.message,
          state: response.state,
          triageLevel: response.triage_level
        }));
      }
    } catch (error) {
      console.error(`Error handling message for ${callId}:`, error);
      ws.send(JSON.stringify({
        type: 'error',
        message: 'Technical difficulties. Connecting you with help.'
      }));

      await this.toolExecutor.handoffToHuman(callState, 'Technical error');
    }
  }

  async executeAction(callId, action, response) {
    const { callState } = this.activeCalls.get(callId);

    switch (action) {
      case 'trigger_emergency_workflow':
        await this.toolExecutor.triggerEmergencyWorkflow(callState);
        break;
      case 'schedule_appointment':
        await this.toolExecutor.scheduleAppointment(callState);
        break;
      case 'handoff_human':
        await this.toolExecutor.handoffToHuman(callState, response.message);
        break;
    }
  }
}

// =============================================================================
// EXPRESS SERVER (WINDOWS-SAFE)
// =============================================================================

const app = express();
app.use(express.json());

// Root dashboard
app.get('/', (req, res) => {
  const status = {
    service: 'ApneDoctors Voice AI Backend',
    platform: 'windows',
    version: '1.0.0',
    status: 'running',
    port: CONFIG.PORT,
    timestamp: new Date().toISOString(),
    endpoints: {
      health: '/health',
      websocket: `ws://localhost:${CONFIG.PORT}`,
      webhook: '/webhook/:action'
    },
    components: {
      stt: 'Transformers.js Whisper',
      llm: 'Ollama Llama 3',
      tts: 'Piper (Windows)',
      websocket: 'Real-time voice'
    },
    features: [
      'Medical AI conversations',
      'Emergency triage detection',
      'Real-time voice processing',
      'HIPAA-compliant logging'
    ]
  };

  // HTML response for browser
  const html = `
<!DOCTYPE html>
<html>
<head>
    <title>ApneDoctors Voice AI Backend</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; background: #f5f5f5; }
        .container { max-width: 800px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
        .status { background: #27ae60; color: white; padding: 10px; border-radius: 5px; margin: 10px 0; }
        .endpoint { background: #ecf0f1; padding: 10px; margin: 5px 0; border-radius: 3px; }
        .feature { background: #3498db; color: white; padding: 8px; margin: 3px 0; border-radius: 3px; display: inline-block; }
        .footer { margin-top: 30px; padding-top: 20px; border-top: 1px solid #bdc3c7; color: #7f8c8d; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ©º ApneDoctors Voice AI Backend</h1>
        <div class="status">âœ… Status: Running on Windows</div>

        <h2>ğŸ“Š System Info</h2>
        <p><strong>Platform:</strong> ${status.platform}</p>
        <p><strong>Port:</strong> ${status.port}</p>
        <p><strong>Version:</strong> ${status.version}</p>
        <p><strong>Cost:</strong> $0/month</p>

        <h2>ğŸ”— Endpoints</h2>
        <div class="endpoint"><strong>Health Check:</strong> <a href="/health">/health</a></div>
        <div class="endpoint"><strong>WebSocket:</strong> ws://localhost:${status.port}</div>
        <div class="endpoint"><strong>Webhook:</strong> /webhook/:action</div>

        <h2>ğŸ¤– AI Components</h2>
        <div class="endpoint"><strong>STT:</strong> ${status.components.stt}</div>
        <div class="endpoint"><strong>LLM:</strong> ${status.components.llm}</div>
        <div class="endpoint"><strong>TTS:</strong> ${status.components.tts}</div>
        <div class="endpoint"><strong>Voice:</strong> ${status.components.websocket}</div>

        <h2>âœ¨ Features</h2>
        ${status.features.map(f => `<div class="feature">${f}</div>`).join(' ')}

        <div class="footer">
            <p><strong>Ready for medical consultations!</strong></p>
            <p>Connect your frontend to start voice AI conversations.</p>
            <p>Last updated: ${new Date().toLocaleString()}</p>
        </div>
    </div>
</body>
</html>`;

  res.send(html);
});

// Health check
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    platform: 'windows',
    models: 'transformers.js + ollama + piper',
    cost: '$0/month',
    timestamp: new Date().toISOString()
  });
});

// Optional: Webhook endpoint for external integrations
app.post('/webhook/:action', async (req, res) => {
  try {
    const { action } = req.params;
    const payload = req.body;

    console.log(`Webhook received: ${action}`, payload);

    // Forward to n8n if configured
    if (CONFIG.N8N_WEBHOOK_URL) {
      await axios.post(`${CONFIG.N8N_WEBHOOK_URL}/${action}`, payload);
    }

    res.json({ success: true, action, timestamp: new Date().toISOString() });
  } catch (error) {
    console.error('Webhook error:', error);
    res.status(500).json({ error: error.message });
  }
});

// =============================================================================
// START SERVER
// =============================================================================

const server = app.listen(CONFIG.PORT, () => {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš€ ApneDoctors Voice AI - WINDOWS-NATIVE VERSION         â•‘
â•‘  Running on port ${CONFIG.PORT}                                  â•‘
â•‘  ğŸªŸ Platform: Windows                                        â•‘
â•‘  ğŸ’° Monthly API costs: $0                                 â•‘
â•‘  ğŸ†“ Using: Transformers.js + Ollama + Piper               â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  `);

  console.log(`
ğŸ“‹ Prerequisites Check:
âœ… Node.js installed
${process.platform === 'win32' ? 'âœ… Running on Windows' : 'âŒ Not running on Windows'}
âœ… Ollama running (ollama serve)
âœ… Piper downloaded (piper.exe in PATH)
âœ… Transformers.js installed (npm install @xenova/transformers)

ğŸ”§ To test:
1. Open browser to: http://localhost:${CONFIG.PORT}/health
2. Connect WebSocket to: ws://localhost:${CONFIG.PORT}
3. Send audio messages in base64 format

ğŸ“š Next steps:
1. Download Piper: https://github.com/rhasspy/piper/releases
2. Get voice model: en_US-lessac-medium.onnx
3. Test with frontend integration
  `);
});

new VoiceWebSocketServer(server);

module.exports = { VoiceWebSocketServer, WindowsVoiceAI, CallStateManager };